{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, Dict,Annotated ,List\n",
    "from langgraph.graph import StateGraph ,START,END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph.message import  add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import  MessagesState\n",
    "from langchain_core.messages import HumanMessage,SystemMessage ,AnyMessage, AIMessage\n",
    "from pydantic import  BaseModel,Field\n",
    "from Prompts import  Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(messages):\n",
    "    print(\"===Pritning state , hold tight=======\") \n",
    "    print(state)\n",
    "    print(\"===The loop isn't looping please wait===\") \n",
    "    for msg in messages:\n",
    "        print(\"=======\")\n",
    "        print(msg)\n",
    "        print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"LNDn2rbGUIGZznn1NXT7U4VcADf-d\"\n",
    "ENDPOINT=\"https://cloud.olakrutrim.com/v1\"\n",
    "MODEL_NAME=\"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "llm = ChatOpenAI(api_key=API_KEY, base_url=ENDPOINT, model=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buisness need \n",
    "Gather and Analyze business requirments \n",
    "Deffine scope and Granularity \n",
    "Identify Fact Tables and Their Measures \n",
    "Define Dimensions and Conformed Dimensions\n",
    "Design the Star Schema Structure\n",
    "Optimize for Performance and Scalability\n",
    "Validate, Test, and Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuisnessNeedPreProcessed(BaseModel):    \n",
    "    messages:Optional[Annotated[List[AnyMessage], add_messages, Field(description=\"Conversations as stored\")]]=None\n",
    "    processed_technical_conversions:Optional[Annotated[List[str],Field(description=\"\")]]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GranularityScope(BaseModel):\n",
    "    conversions:Annotated[str, Field(description=\"Conversations as stored\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaDesign(BaseModel):\n",
    "    conversions:Annotated[str, Field(description=\"Conversations as stored\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceOptimization(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErDiagram(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codd State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoddState(BaseModel):\n",
    "    business_need: Optional[Annotated[BuisnessNeedPreProcessed, Field(description=\"Original raw business need text as provided by the client\")]] =None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_node(state: CoddState):\n",
    "    i = 1\n",
    "    while i != 0:\n",
    "        print(state.business_need.messages)\n",
    "        response = llm.invoke(state.business_need.messages)\n",
    "        print(response)\n",
    "        i = i - 1\n",
    "        \n",
    "    return {\n",
    "        \"business_need\":{ \n",
    "            \"messages\": state.business_need.messages\n",
    "        }   \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges of Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_function(state:CoddState):\n",
    "    if state.business_need and state.business_need.messages:\n",
    "        last_message=state.business_need.messages[-1]\n",
    "        if last_message is AIMessage:\n",
    "            # suppose  last message is a string\n",
    "            if last_message.content.contains(\"EXIT\"):\n",
    "                return END\n",
    "    \n",
    "    return \"conversation_node\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_234 = {\"configurable\": {\"thread_id\": \"abc234\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worflow added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=CoddState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7e2068d5c7c0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"conversation_node\", conversation_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7e2068d5c7c0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(START, \"conversation_node\")\n",
    "workflow.add_conditional_edges(\"conversation_node\", routing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complie the Workflow with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Run it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on same thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on sperate Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"You're ready to share your business needs. I'm all ears. Please go ahead and tell me what you're looking to achieve with your OLAP data store. What kind of insights are you hoping to gain, and what problems are you trying to solve?\\n\\nDon't worry if it's still a bit fuzzy - we can work through it together. I'll ask some questions to help clarify things, and before you know it, we'll have a clear plan in place.\\n\\nSo, what's on your mind? What are your top priorities for your OLAP data store?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 580, 'total_tokens': 696, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-83d19b45-f434-4528-bf5f-0f93de1853bd-0' usage_metadata={'input_tokens': 580, 'output_tokens': 116, 'total_tokens': 696, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"You're ready to share your business needs. I'm all ears. Please go ahead and tell me what you're looking to achieve with your OLAP data store. What kind of insights are you hoping to gain, and what problems are you trying to solve?\\n\\nDon't worry if it's still a bit fuzzy - we can work through it together. I'll ask questions to help clarify things, and before you know it, we'll have a clear picture of what you need.\\n\\nSo, what's on your mind? What are your business needs?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 580, 'total_tokens': 691, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-b909585c-5be4-4d3b-bcae-a5301def8075-0' usage_metadata={'input_tokens': 580, 'output_tokens': 111, 'total_tokens': 691, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content='Let\\'s dive into understanding those business needs. You mentioned wanting to analyze your online store\\'s performance, but I\\'d love to break it down further.\\n\\nTo get started, could you tell me what you mean by \"understanding your online store\\'s performance\"? Are you looking to track sales, website traffic, customer behavior, or something else?\\n\\nAlso, what kind of time frame are you interested in analyzing? Are you looking at daily, weekly, monthly, or yearly trends?\\n\\nAnd when it comes to comparing different product categories, are there any specific categories you\\'re most interested in? For example, are you looking at electronics, clothing, home goods, or something else?\\n\\nLastly, how do you define a \"successful\" online store? Is it based on revenue, customer satisfaction, or some other metric?\\n\\nLet\\'s chat about these details, and I\\'ll do my best to help you refine your business needs. I\\'m all ears!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 188, 'prompt_tokens': 580, 'total_tokens': 768, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-19477b19-b95d-471c-8d21-7f852c5f4983-0' usage_metadata={'input_tokens': 580, 'output_tokens': 188, 'total_tokens': 768, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content='Let\\'s dive into understanding those business needs. You\\'ve shared that you want to analyze your online store\\'s performance, but I\\'d love to break it down further to ensure we capture everything accurately.\\n\\nTo start, could you tell me what you mean by \"understanding sales trends\"? Are you looking at daily fluctuations, weekly patterns, or perhaps monthly summaries? Knowing the timeframe will help us design a more tailored approach.\\n\\nAlso, when it comes to comparing different product categories, are there specific categories you\\'re most interested in? For example, are you looking at electronics, clothing, home goods, or something else?\\n\\nLastly, defining a \"loyal customer\" can vary from business to business. How do you envision loyalty in the context of your online store? Is it based on the number of repeat purchases, the total amount spent, or maybe the frequency of visits to your site?\\n\\nLet\\'s chat about these details so we can create a clear picture of what you\\'re aiming to achieve!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 580, 'total_tokens': 778, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-377422a0-9f1b-477c-89ad-d3d080406cb1-0' usage_metadata={'input_tokens': 580, 'output_tokens': 198, 'total_tokens': 778, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"You're ready to dive into your business needs. I'm all ears. Please go ahead and share what's on your mind. What are you hoping to achieve with your OLAP data store? Are you looking to analyze sales, customer behavior, marketing effectiveness, or something else?\\n\\nRemember, I'm here to listen and help. Don't worry if your ideas are still rough around the edges – we can refine them together. So, what's the main goal you have in mind for your OLAP data store?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 580, 'total_tokens': 684, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a41236bd-96a6-4bea-9154-41a2e8663c03-0' usage_metadata={'input_tokens': 580, 'output_tokens': 104, 'total_tokens': 684, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"I'm excited to dive into the business needs with you. To get started, could you please share what your business needs are? What are you trying to achieve or analyze with your OLAP data store? \\n\\nPerhaps you're looking to understand customer behavior, track sales performance, or optimize inventory management? The more details you can provide, the better I'll be able to assist you in refining your business needs and designing an effective OLAP data store.\\n\\nIf you're not sure where to begin, don't worry! We can break it down together. Just give me a general idea of what's on your mind, and we'll take it from there.\\n\\n(And don't worry, I'll ask plenty of questions to ensure I understand your needs correctly. I might even throw in a few playful remarks to keep things engaging!)\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 580, 'total_tokens': 746, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ede78fa6-c4dc-406d-ac79-1db19a482f53-0' usage_metadata={'input_tokens': 580, 'output_tokens': 166, 'total_tokens': 746, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"You want to share the business needs for designing an OLAP data store. I'm all ears. Please go ahead and share what's on your mind. What are you trying to achieve with your data store? What kind of insights are you hoping to gain?\\n\\n(And don't worry if it's still a bit fuzzy - we can work through it together. I'll ask questions to help clarify things and make sure we're on the same page.)\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 580, 'total_tokens': 671, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-b07c5e5a-cedc-4acb-bde5-44ee0db68963-0' usage_metadata={'input_tokens': 580, 'output_tokens': 91, 'total_tokens': 671, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"You're ready to dive into your business needs. I'm all ears. Please go ahead and share what you're looking to achieve with your OLAP data store. What kind of insights are you hoping to gain, and what problems are you trying to solve?\\n\\nDon't worry if it's still a bit fuzzy – we can work through it together. I'll ask some questions to help clarify things, and before you know it, we'll have a solid understanding of what you need.\\n\\nSo, what's on your mind? What are your top priorities for your OLAP data store?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 580, 'total_tokens': 698, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-e07be1eb-5ce8-4666-9555-78aadb9e403c-0' usage_metadata={'input_tokens': 580, 'output_tokens': 118, 'total_tokens': 698, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [SystemMessage(Prompts\u001b[38;5;241m.\u001b[39mgen_analysis_business_need()), HumanMessage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat about the business needs \u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbusiness_need\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_234\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2124\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2123\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2126\u001b[0m     config,\n\u001b[1;32m   2127\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   2128\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2129\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   2130\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   2131\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2133\u001b[0m ):\n\u001b[1;32m   2134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2135\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1779\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1779\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1780\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1781\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1782\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1783\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1784\u001b[0m         ):\n\u001b[1;32m   1785\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/utils/runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/utils/runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m, in \u001b[0;36mconversation_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state\u001b[38;5;241m.\u001b[39mbusiness_need\u001b[38;5;241m.\u001b[39mmessages)\n\u001b[0;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbusiness_need\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m      8\u001b[0m     i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:783\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 783\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1278\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m     )\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/openai/_base_client.py:1003\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1000\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1009\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_messages = [SystemMessage(Prompts.gen_analysis_business_need()), HumanMessage(\"What about the business needs \")]\n",
    "try:\n",
    "    output = graph.invoke({\"business_need\": {\"messages\":input_messages}}, config_234)\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
