{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, Dict,Annotated ,List\n",
    "from langgraph.graph import StateGraph ,START,END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph.message import  add_messages\n",
    "from pydantic import  BaseModel,Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"LNDn2rbGUIGZznn1NXT7U4VcADf-d\"\n",
    "ENDPOINT=\"https://cloud.olakrutrim.com/v1\"\n",
    "MODEL_NAME=\"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "llm = ChatOpenAI(api_key=API_KEY, base_url=ENDPOINT, model=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buisness need \n",
    "Gather and Analyze business requirments \n",
    "Deffine scope and Granularity \n",
    "Identify Fact Tables and Their Measures \n",
    "Define Dimensions and Conformed Dimensions\n",
    "Design the Star Schema Structure\n",
    "Optimize for Performance and Scalability\n",
    "Validate, Test, and Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage,SystemMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.graph.message.MessagesState'>\n"
     ]
    }
   ],
   "source": [
    "print(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    \n",
    "    def pretty_print(messages):\n",
    "        print(\"===Pritning state , hold tight=======\") \n",
    "        print(state)\n",
    "        print(\"===The loop isn't looping please wait===\") \n",
    "        for msg in messages:\n",
    "            print(\"=======\")\n",
    "            print(msg)\n",
    "            print(\"=======\")\n",
    "    \n",
    "    pretty_print(state[\"messages\"])\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7b26993993f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "from Prompts import  Prompts\n",
    "print(type(Prompts.gen_analysis_business_need()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n                    '), additional_kwargs={})]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatPromptTemplate' object has no attribute 'tamplate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi! I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Bob.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Prompts\u001b[38;5;241m.\u001b[39mgen_analysis_business_need())\n\u001b[0;32m----> 3\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [SystemMessage(\u001b[43mPrompts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_analysis_business_need\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtamplate\u001b[49m),HumanMessage(query)]\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_messages}, config)\n\u001b[1;32m      5\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()  \n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatPromptTemplate' object has no attribute 'tamplate'"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Hi! I'm Bob.\"\n",
    "print(Prompts.gen_analysis_business_need())\n",
    "input_messages = [SystemMessage(Prompts.gen_analysis_business_need().tamplate),HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "content=\"Hi! I'm Bob.\" additional_kwargs={} response_metadata={} id='93a8fdf7-2cdf-4773-afcc-1d16f23731e9'\n",
      "=======\n",
      "=======\n",
      "content=\"Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-698baf98-8a56-41b1-b448-61ddb1a8c414-0' usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67, 'input_token_details': {}, 'output_token_details': {}}\n",
      "=======\n",
      "=======\n",
      "content=\"What's my name?\" additional_kwargs={} response_metadata={} id='ede12a17-b844-4a61-9c3f-3783aeeb9217'\n",
      "=======\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. You told me that when we started talking.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "content=\"What's my name?\" additional_kwargs={} response_metadata={} id='d115655e-b9f1-4181-b835-03e0290aacd6'\n",
      "=======\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. I'm a large language model, I don't have the ability to access personal information about you, and we haven't interacted before, so I don't have any context about your identity. If you'd like to share your name with me, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "content=\"Hi! I'm Bob.\" additional_kwargs={} response_metadata={} id='93a8fdf7-2cdf-4773-afcc-1d16f23731e9'\n",
      "=======\n",
      "=======\n",
      "content=\"Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-698baf98-8a56-41b1-b448-61ddb1a8c414-0' usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67, 'input_token_details': {}, 'output_token_details': {}}\n",
      "=======\n",
      "=======\n",
      "content=\"What's my name?\" additional_kwargs={} response_metadata={} id='ede12a17-b844-4a61-9c3f-3783aeeb9217'\n",
      "=======\n",
      "=======\n",
      "content='Your name is Bob. You told me that when we started talking.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 81, 'total_tokens': 96, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4693a88d-2ea7-4135-a351-611794ec2115-0' usage_metadata={'input_tokens': 81, 'output_tokens': 15, 'total_tokens': 96, 'input_token_details': {}, 'output_token_details': {}}\n",
      "=======\n",
      "=======\n",
      "content=\"What's my name?\" additional_kwargs={} response_metadata={} id='f0b3ed68-66cc-43e9-8af4-f97fb65ea4aa'\n",
      "=======\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is still Bob! You introduced yourself as Bob at the beginning of our conversation.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuisnessNeedPreProcessed(BaseModel):\n",
    "    conversions:Annotated[List[str], Field(description=\"Conversations as stored\")]\n",
    "    processed_technical_conversions:Annotated[List[str],Field(description=\"\")]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation_chain_with_memory(buisness_need:str):\n",
    "    memory= ConversationSummaryBufferMemory()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GranularityScope(BaseModel):\n",
    "    conversions:Annotated[str, Field(description=\"Conversations as stored\")]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaDesign(BaseModel):\n",
    "        conversions:Annotated[str, Field(description=\"Conversations as stored\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceOptimization(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErDiagram(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoddState(BaseModel):\n",
    "    business_need: Annotated[str, Field(description=\"Original raw business need text as provided by the client\")]\n",
    "    business_need_pre_processed: Annotated[str, Field(description=\"The pre-processed version of the business need with key points extracted\")]\n",
    "    granularity_scope: Annotated[str, Field(description=\"Defined granularity and scope of the business process for the schema\")]\n",
    "    schema_design: Annotated[str, Field(description=\"The designed star schema including fact and dimension definitions\")]\n",
    "    performance_optimization: Annotated[str, Field(description=\"Details on indexing, partitioning, and aggregation strategies to optimize performance\")]\n",
    "    er_diagram: Annotated[str, Field(description=\"Information about the generated ER diagram for the schema\")]\n",
    "    documentation: Annotated[str, Field(description=\"Documentation summarizing the design, decisions, and maintenance guidelines\")]\n",
    "    current_stage: Annotated[str, Field(description=\"Current processing stage in the Codd pipeline\")]\n",
    "    error: Annotated[str, Field(description=\"Defined granularity and scope of the business process for the schema\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages:Annotated[list,add_messages]\n",
    "\n",
    "\n",
    "graph_builder= StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47889/891791678.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory)\n",
      "/tmp/ipykernel_47889/891791678.py:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = conversation.run(initial_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: To clarify your requirements, can you please tell me what specific data entities from the ERP and CRM systems you would like to integrate into the data warehouse? For example, are you looking to include customer information, sales orders, inventory levels, financial transactions, or something else? Additionally, what is the approximate volume of data we're talking about, and what is the desired frequency of data updates - real-time, daily, weekly, or monthly? \n",
      "\n",
      "Also, what type of reporting and decision-making are you trying to support with this data warehouse? Are you looking to create dashboards, generate ad-hoc reports, or perform advanced analytics and data mining? \n",
      "\n",
      "Lastly, do you have any existing data warehouse or business intelligence tools that we should consider integrating with, such as Tableau, Power BI, or Oracle OBIEE? Or would you like me to recommend some options?\n",
      "LLM: It seems like you're ready to provide more information. To further clarify your requirements, can you please tell me about the current data structures and formats used by your ERP and CRM systems? For example, are they relational databases like MySQL or PostgreSQL, or are they cloud-based systems like Salesforce or Microsoft Dynamics? \n",
      "\n",
      "Additionally, do you have any specific data governance or security requirements that need to be considered when designing the data warehouse, such as data encryption, access controls, or compliance with regulations like GDPR or HIPAA?\n",
      "\n",
      "Also, what is the expected user base for the data warehouse? Will it be used by a small team of power users, or will it be rolled out to a larger group of employees across the organization? This will help me understand the scalability and usability requirements for the data warehouse.\n",
      "\n",
      "Lastly, are there any specific key performance indicators (KPIs) or metrics that you would like to track and analyze through the data warehouse, such as customer satisfaction, sales revenue, or inventory turnover?\n",
      "LLM: It seems like you're ready to provide more information. To further clarify your requirements, can you please tell me about the desired data warehouse architecture? For example, would you like to use a traditional relational database management system like Oracle or SQL Server, or would you like to consider a more modern cloud-based data warehousing solution like Amazon Redshift, Google BigQuery, or Snowflake?\n",
      "\n",
      "Additionally, would you like to use an Extract, Transform, Load (ETL) tool like Informatica or Talend to manage the data integration process, or would you like to consider a more real-time data integration approach using tools like Apache Kafka or Apache NiFi?\n",
      "\n",
      "Also, have you considered how you would like to handle data storage and scalability for the data warehouse? For example, would you like to use a scalable storage solution like Hadoop or a cloud-based object storage like Amazon S3, or would you like to consider a more traditional storage approach using SAN or NAS storage?\n",
      "\n",
      "Lastly, are there any specific data visualization or reporting tools that you would like to use to interact with the data warehouse? For example, would you like to use a tool like Tableau or Power BI to create interactive dashboards, or would you like to consider a more custom approach using a programming language like Python or R?\n",
      "\n",
      "And one more thing, do you have a rough estimate of the project timeline and budget for the data warehouse implementation? This will help me understand the constraints and prioritize the requirements accordingly.\n",
      "LLM: It seems like you're looking to continue the conversation, but I had already gathered enough information and replied with \"STOP\" as per your initial instruction. If you'd like to discuss further or clarify any points, I'm happy to continue the conversation. What would you like to talk about?\n",
      "LLM decided to stop asking questions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "initial_prompt = (\n",
    "    \"We need to build a data warehouse that integrates data from our ERP and CRM systems \"\n",
    "    \"to improve reporting and decision-making. Ask me any questions to clarify our requirements. \"\n",
    "    \"When you have all the information you need, simply reply with 'STOP'.\"\n",
    ")\n",
    "\n",
    "response = conversation.run(initial_prompt)\n",
    "print(\"LLM:\", response)\n",
    "\n",
    "while True:\n",
    "    # Ask the next question or prompt\n",
    "    next_output = conversation.run(\"Next question?\")\n",
    "    print(\"LLM:\", next_output)\n",
    "    \n",
    "    # Check if the LLM signals that it is done\n",
    "    if \"STOP\" in next_output.upper():\n",
    "        print(\"LLM decided to stop asking questions.\")\n",
    "        break\n",
    "\n",
    "    # (Optional) Simulate user responses here. In a real application, you might capture user input.\n",
    "    user_input = input(\"Your answer: \")\n",
    "    conversation.run(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
