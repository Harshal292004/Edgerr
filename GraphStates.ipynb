{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, Dict,Annotated ,List\n",
    "from langgraph.graph import StateGraph ,START,END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph.message import  add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import  MessagesState\n",
    "from langchain_core.messages import HumanMessage,SystemMessage ,AnyMessage, AIMessage\n",
    "from pydantic import  BaseModel,Field\n",
    "from Prompts import  Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(messages):\n",
    "    print(\"===Pritning state , hold tight=======\") \n",
    "    print(state)\n",
    "    print(\"===The loop isn't looping please wait===\") \n",
    "    for msg in messages:\n",
    "        print(\"=======\")\n",
    "        print(msg)\n",
    "        print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"LNDn2rbGUIGZznn1NXT7U4VcADf-d\"\n",
    "ENDPOINT=\"https://cloud.olakrutrim.com/v1\"\n",
    "MODEL_NAME=\"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "llm = ChatOpenAI(api_key=API_KEY, base_url=ENDPOINT, model=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buisness need \n",
    "Gather and Analyze business requirments \n",
    "Deffine scope and Granularity \n",
    "Identify Fact Tables and Their Measures \n",
    "Define Dimensions and Conformed Dimensions\n",
    "Design the Star Schema Structure\n",
    "Optimize for Performance and Scalability\n",
    "Validate, Test, and Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuisnessNeedPreProcessed(BaseModel):    \n",
    "    messages:Optional[Annotated[List[AnyMessage], add_messages, Field(description=\"Conversations as stored\")]]=None\n",
    "    processed_technical_conversions:Optional[Annotated[List[str],Field(description=\"\")]]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GranularityScope(BaseModel):\n",
    "    conversions:Annotated[str, Field(description=\"Conversations as stored\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaDesign(BaseModel):\n",
    "    conversions:Annotated[str, Field(description=\"Conversations as stored\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceOptimization(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErDiagram(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codd State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoddState(BaseModel):\n",
    "    business_need: Optional[Annotated[BuisnessNeedPreProcessed, Field(description=\"Original raw business need text as provided by the client\")]] =None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_node(state: CoddState):\n",
    "    print(\"Hello Brother\")\n",
    "    i = 1\n",
    "    while i != 0:\n",
    "        print(state.business_need.messages)\n",
    "        response = llm.invoke(state.business_need.messages)\n",
    "        print(response)\n",
    "        i = i - 1\n",
    "        \n",
    "    return {\n",
    "        \"business_need\":{ \n",
    "            \"messages\": state.business_need.messages\n",
    "        }   \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges of Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_function(state:CoddState):\n",
    "    if state.business_need and state.business_need.messages:\n",
    "        last_message=state.business_need.messages[-1]\n",
    "        if last_message is AIMessage:\n",
    "            # suppose  last message is a string\n",
    "            if last_message.content.contains(\"EXIT\"):\n",
    "                return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_234 = {\"configurable\": {\"thread_id\": \"abc234\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_node\u001b[39m\u001b[38;5;124m\"\u001b[39m, conversation_node)\n\u001b[1;32m      4\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_conditional_edges(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_node\u001b[39m\u001b[38;5;124m\"\u001b[39m, routing_function, {\u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_node\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_node\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 5\u001b[0m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_nodee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mEND\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/graph/state.py:418\u001b[0m, in \u001b[0;36mStateGraph.add_node\u001b[0;34m(self, node, action, metadata, input, retry, destinations)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_schema(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[cast(\u001b[38;5;28mstr\u001b[39m, node)] \u001b[38;5;241m=\u001b[39m StateNodeSpec(\n\u001b[0;32m--> 418\u001b[0m     \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m    419\u001b[0m     metadata,\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema,\n\u001b[1;32m    421\u001b[0m     retry_policy\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m    422\u001b[0m     ends\u001b[38;5;241m=\u001b[39mends,\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Edgerr/.venv/lib/python3.10/site-packages/langgraph/utils/runnable.py:429\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing, name, trace)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableParallel(thing)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(state_schema=CoddState)\n",
    "workflow.add_edge(START, \"coversation_node\")\n",
    "workflow.add_node(\"conversation_node\", conversation_node)\n",
    "workflow.add_conditional_edges(\"conversation_node\", routing_function, {True: \"next_node\", False: \"conversation_node\"})\n",
    "workflow.add_node(\"next_nodee\",END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complie the Workflow with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGsAAACGCAIAAABVB+MHAAAAAXNSR0IArs4c6QAAD4ZJREFUeJztnXlwE9cdgJ+0OleXdRjhE9uAMWBjCCY1YIIBQ4lj7HgINQWnJA20tEx6QNo0MxCSMkMSN9PSgSlJC6ElOCEkkLoKGXACmMMOhyEF25jLsrEtCXRrtbp3pf4halKse1do7er7z9r33v786e3qXbuP5vP5QBIC0BMdwIgnaZAoSYNESRokStIgUZIGicIgmN9q9FgMHrsVtyM45vF5vSOgbQQxAINBh4UQLGCIxzJhPiEJtNjagwaNq+earbfDxoJpwEeDBRAshLg8hhcfAQYZTBqKYHYEt1sxl8PLZNHzingTivlCKTOG0qI2iJqxNoXeB0CKjJlbxBuTyYnhrJRC0+tQdthM9918MWNOlYzFie7OFp3BS83GzjbLnGWySTMF0YdKdTrOWdq+0Jc+Iy2elxJ5rigMNu1WTZjBn1oqijXCkcHlr42Ge+4l9WMjTB9pjd27pXfGQvGo1wcAmFkhGVfAa9qtijSDLwL2bFbq1c5IUo4abv/bevDd/khShr+Km3arZiwUZ0+CSfh+RxTdFxCV0lHxQ3noZGEMtn9l5PKhqbNH/8UbkPavjVxemH8/1H0QNWMdrZb/W30AgJIKyalDutBpQhlsU+jnLJORHdUIY3aVtE2hD5EgqEGDxuUDYFS2+6Ji5iKxXu1y2rBgCYIa7LlmS5HF0suJjc7OTpfLlajsoeEJGcpOe7CjQQ32dthyi3hxiukRFArFCy+84HA4EpI9LHlFfGUHGuxoYIOI0cOG6Y+tzxtz9fE3JOJX+/zkFvJQExZs2CmIQYMnTlN4d+/eXb9+fVlZWWVl5fbt271er0KhePvttwEAFRUVJSUlCoUCAHD//v2tW7dWVFSUlpbW1dUdO3bMn91sNpeUlHz44YebN28uKytbt25dwOykg3l8Fr0n4KHAQ2N2Kw4LoHiEsm3btr6+vk2bNtlstvb2djqdPnfu3Pr6+gMHDuzYsYPP52dnZwMAMAzr6up67rnnUlJSTp48uXnz5qysrKlTp/oL2bt374oVK9577z0IguRy+fDspAMLITuCi8cEOBTEIILDwrgYVKvVBQUFtbW1AID6+noAgEQiyczMBAAUFhampDwYFMnIyPj0009pNBoAoKampqKioqWlZchgUVHRhg0bhsocnp10eEKGDQn8cxz0l4TJissEQGVl5fnz5xsaGoxGY+iUt27d2rhx49KlS2tra3EcNxgMQ4eefPLJeMQWAhaHHqzzFlgTh0e3moK2gIiwYcOGjRs3Njc3V1dXHzp0KFiyS5curVmzxu12b926taGhQSQSeb3eoaNcLjcesYXAovfAgsDXa+BPYQHDbo2LQRqNtmrVqpqamu3btzc0NOTn50+fPt1/6Ltf8p49ezIzM3fs2MFgMCJUFtflKyF+GALXQb4YYnPjchX7Wx48Hm/9+vUAgBs3bgwJ0uke9kDNZnN+fr5fn9vtttvt362DjzA8O+nwRJBAHLh/EbgOSuRs3aDbrHOnpLLIDeXVV1/l8/mlpaXnzp0DAEyePBkAUFxcDEHQu+++W11d7XK5li9f7m+XNDU1iUSixsZGBEF6enqC1bLh2cmNWXXH4cVAsPkT6I033gh4wGrCbBYsLZfkO87g4OC5c+eOHTvmcDhefvnl8vJyAIBQKJTL5V999dXZs2cRBKmqqiouLlYqlQcPHmxvb1+8eHFdXd3x48cLCgqkUun+/fvLysqmTJkyVObw7OTGfPW0WZ7DGZsTuH8RdHxQrXR0X0AWhRtf/H/g6F5NWY1MFGSUIOhkc3oe9+Ix48Ate1Z+4NFpBEGqq6sDHsrMzBwcHBz++fz58998882II4+RtWvX3rlzZ/jnkydP7u7uHv55YWHhrl27gpXWfRFhc+nB9IUZo9YOOE8d0tVtygp41Ov13rt3L3ChtMDFcrlcsVgc7HRkodPpPJ4APbBgUbFYLJks6DDo3i29P/xtVrCmTPhR/jOf67Lz4Zypj2mQhmp0nbfYEXzWEkmINGGaLE/Vpp4+okMMgTvVoxt1j+PGJWtofSCS2U6XE3/vt3fImEEcSThsnvd/1xNJyojmi90u/P3X7qAWD+HARgbaQefe15UY5o0kcaSrPhwo/nFD//d/JM+YMMonju9ctbY3m1b+JtJRsuhWHp36RIuYPHOXyWQZ7FgjpC6qHsc3CoN8HHtebWrkuaJe/dZ/w96q0GcXwPIsTm4hD2LQog+VWridXmUneq/PadS4Zy+TpuVE1w2LcQVmzzX01hVrb6dt0kwBk03nCRk8EcSBoZGwhBVAdJrditkQzIbgqMUzeMuRV8jPL+GPK4il0RajwSH6b9hNWrcNwWwW3Ov1YW4yFeI43tHRMTT8RRZsmO4fduYJIWkai+CdnajBuIKiaFVVVUtLS6IDCUVyLT9RkgaJQnWD/iFYKkN1gwHHoygF1Q3GbwqYLKhu0Gw2JzqEMFDdYHp6eqJDCAPVDarV6kSHEAaqGywqKkp0CGGgusGOjo5EhxAGqhukPlQ3GGIWjSJQ3aBeH+pJBCpAdYOpqVEMFycEqhuM64osUqC6QepDdYMTJkxIdAhhoLrBgGuIKAXVDVIfqhv87kpLakJ1g9evX090CGGgukHqQ3WDybEZoiTHZkY/VDeYnO0kSnK2c/RDdYPJ+WKiJOeLiTJx4sREhxAGqhu8fft2okMIA9UNUh+qGxw7NtJ3USYKqhsM9vAjdaC6wcLCwkSHEAaqG+zs7Ex0CGGgusFkHSRKsg4SJSsr8BP21IGKT+SsW7dOrVYzGAyv16vX62UyGZ1O93g8X375ZaJDCwAV6+Dq1asRBFGpVBqNxuPxaDQalUoFQXF5kxpxqGiwvLz8ke6wz+ej7IQJFQ0CAJ5//nkYfvjAYFpa2sqVKxMaUVAoanDBggW5ublD9+ji4uJp06YlOqjAUNQgAODFF1/0D6/KZDLKVkBKGywvL8/Ly/NPGVP2JkjCPk2RgHu8DpvXjmBOO45F81bDZ5f81GX6pLL8RWWnLfJcDCaNy4NgIQTzIRo97i8xiGN70Kxz93XZb32Lelw+uxVjcSG+mONyxOXFkN+FyYJsFpfbgfPFTA5Mz5/OGzcFDvb2QOLExaBJ6z5zxGAxYGw+my+DeZLH/dLPIax6O6q3e90e6VjmvFopT0j+NUe+wa8/0t29aU/NEwvHUOhtXWa1VdtjmlIqLKuWklsymQYdKH7grf7U8ZKUND5ZZZKLSYXYdNb618h8ZzVpBq0mz0fvDOSVZjDZj+PXKWYciKvnvPonb+VFu6taMMgxaNC4ju7TZs+g+pOsfnw+3912dd2mdC6PhC+bhO/B6/V9/IeBkaLP/yrHjGnyxrcGyCmNeB08vFPFT5OyeY9vMxNSsJscPgf6zEtE5wKJ1sHLJ0yYjzni9AEAYDHXbPTd/tZKsByiBs8fNYwZH+4tkVQldbz47D8NESQMBSGDl5qNaQWSx9BzihMsLlMo53V9YyFSCCGDHa0IX/a4m80X2pte2fI9BAn11KzNZn5ly/faLh4OWxo3Be5oI3Qhx25Qr3bRIDqLS+nWX1h4Yo5Z63ba8JhLiN1gbyfKl42GN4qmpMF9XVGM/TxC7DVI0+dmweEv4X2NvxmTmuPxONu/Perz+SaOn1U2u+7E6X19/dcEfOn3F/5k5vSn/SnvDnR+cXzngOo6i8WdOmnesqW/hGGh/5BKffOfX/5xQHVdKJClSv+nT9Z28fDp1o8siFYiTp8xbUn53HomM7o3nDK5rHv9roJYN42JvQ7aEYzJjmj+7NTZ/QCA9T/+S3lZfWf36b/94xeFBfN/9uPd6Wn5B4/8/r6uDwBwT6t8f98GHPfU1W5ZXP5SR3fL/k9e82e/r+vb/cHPEERXufjn8+esUmluDpXcfPJvR4/vml60+AfPbp42dVHL2QOfNb0V7T/CYDOIbGcTex10oLiEFZFBeWrus89sAgBkphdcuPyv7Mwpc0tXAABqnv515/UWZe8VeWrOiZZ9NBp93Y/+zOUKAAAwV/jx4Td6eq+Mz33i6PGdNBr95Z/u5fPEAAAanX5E0QAAsCC6E2f+vvq5bdMKF/pPJBLIDiveqancGNU/wmBBVkPs98HYDXL4DDojoirMYDy8rJhMNgQ9aH6niOQAAJvdDADo6bsyIa/Erw8AMGliKQBgQNWdlTHl5p3zs2ct9+sDAED0BzHf7rmI41jjZ683fvb6f4v3AQAsVq2QH8UbQiAmncmO/VqM3SDm9mIunBFZNQyIfyswf7fS6USHHAEAuBwhAACx6hGrHscxiThteHbEqgcAvFT/xxTR/+yCJpVkOp1BtzgcjseJAQJd29gNwgIIc8de+R9BJBxjczxs2aI2IwCAy+H7taKoaXgWLvfB78yY1Bwip8bcOF8Uu4fYa68sg417SDM4LrtI2XvF7Xb6/7zWdRIAkDtuOofDk0mzrnadwLBHd1eYmFdCo9HOXXi44ZjL/WDrTv+Nwu5AIjm1F/dJ02Pv18ducOw4FqoPuqNqtCx66gWX27Fn/6+uXD1+8sw/jjbvmpA7c3zuEwCAJQvWGoyDO/+6tvX8p20XD7e0NvqzyKRZZaV112+c/eDApguX//V1ywdv/2n5oPoGAIDD4UklmWdaP/rm0udhT41q0XQCmynFbjCvkI9oSTOYKstet+bPGO755PNtLa2NM4ufXrOqwX+jfKJ4ae0zr9gdli+ad168rBiX9XBNZvXTv1q29Bea+z1HFO9cuNxUOKVcJHxwT1y94vcyafblb8Ms98I9uMPqSR8fu0FC44Nf7LlH4/ITOBVHHLMGFcCuRSsD7cgZGYRGFqbNExoHqP7UVmiM/eYZCwg9ukfIYPYkmMOloYZ4bb0cbywaa3ouWyIntCcf0RHWp2olqDainzwKgupR4tPHRA3Ks7nji7h6ZZiNYCmI5rr2iQVCPuHVICTM1c1aLIZh3KwaSTVR12vMyGNOniUkXhRpM+4nD+nMFkiSKSKltLii7THm5jNmLSFn5zzS1g8u/EEqj+PWK4lO3MSb+ze1aZk0svSRv/LoyinTrSs2/hgRBYevES3qMKLTnxJOmikgsVjy127p1c5WhREx4qJ0kSAV9vcrEogX81oNDlO/aWwOe06VVCgheWo7Xisw1UrH1bOWnn+j4nSYK4YhiMZgM1gcBoi/T5/X53FiHhfu8/lsOtRqcE2aJSguE0nT47K/Wdyfaertsmn7nTqVB7VgEIOO6N1xPR0AQCBh+nw+fgpDnsmS53CC7X1LFlR8KmxkQd21/COFpEGiJA0SJWmQKEmDREkaJMp/AGOWqtryQOtzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Run it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on same thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on sperate Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Brother\n",
      "[SystemMessage(content='\\n                    You are Codd, a knowledgeable and playful chatbot specialized in Database Administration and Data Modeling. Your mission is to help users refine their business needs for designing an OLAP data store by engaging in a friendly, clear, and jargon-free conversation. Your responses should feel warm and human, not robotic. Follow these guidelines:\\n\\n                    1. **Listen Carefully:** When a business need is provided, read it thoroughly.\\n                    2. **Analyze & Inquire:** Break down the business need into its key components and ask simple, thoughtful questions to gather more details. For example:\\n                    - \"Could you explain what you mean by \\'sales trends\\'? Are you looking for daily, weekly, or monthly patterns?\"\\n                    - \"Which product categories do you want to focus on?\"\\n                    - \"How do you define a loyal customer for your business?\"\\n                    3. **Keep It Simple:** Avoid technical jargon; assume the user might not be familiar with database terms.\\n                    4. **Be Engaging:** Maintain a friendly, playful tone. Feel free to add a light humorous remark when appropriate.\\n                    5. **Self-Exit When Done:** Once you have gathered enough clear and useful information to build a perfect schema, kindly indicate that you’re handing off the conversation. To do this, call the tool \"exit_conversation\" (i.e., use the self-call mechanism) so that other agents can take over with the detailed schema design.\\n                    6. **Handle Vague Needs:** If the business need is too vague or lacks useful details, politely ask for clarification or suggest improvements.\\n\\n                    **Example Conversation:**\\n\\n                    - **Business Need:** \"I want to understand my online store\\'s sales trends over time, compare different product categories, and know which customers are most loyal.\"\\n                    \\n                    - **Your Analysis & Questions:**\\n                    - \"Hi there I am Codd! To help me get started, could you tell me what you mean by \\'sales trends\\'? Are you interested in daily, weekly, or monthly patterns?\"\\n                    - \"Great! And which product categories are you most interested in comparing?\"\\n                    - \"One more thing: how do you define a \\'loyal customer\\'? Is it based on repeat purchases, total spend, or something else?\"\\n                    - *After gathering sufficient details:* \"Awesome, I think I have enough information now to craft the perfect schema. I\\'m going to hand things over to the next team—please hold while I call \\'exit_conversation\\'.\"\\n\\n                    Your task is to collect the needed details with clarity and warmth. When you feel all required details have been gathered, gracefully conclude by invoking the exit tool.\\n        ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about the business needs ', additional_kwargs={}, response_metadata={})]\n",
      "content=\"You're ready to share your business needs. I'm all ears. Please go ahead and tell me what you're looking to achieve with your OLAP data store. What kind of insights are you hoping to gain, and what problems are you trying to solve?\\n\\nRemember, I'm here to listen and help, so don't worry if your ideas are still rough around the edges. We'll work together to refine them and get to the heart of what you need.\\n\\nSo, what's on your mind? What are your business needs?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 580, 'total_tokens': 688, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-deaddfe3-1887-4548-8341-49e3c092c3a8-0' usage_metadata={'input_tokens': 580, 'output_tokens': 108, 'total_tokens': 688, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "input_messages = [SystemMessage(Prompts.gen_analysis_business_need()), HumanMessage(\"What about the business needs \")]\n",
    "try:\n",
    "    output = app.invoke({\"business_need\": {\"messages\":input_messages}}, config_234)\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5501/2196735241.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ConversationChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[1;32m      3\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory()\n\u001b[0;32m----> 4\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m(llm\u001b[38;5;241m=\u001b[39mllm, memory\u001b[38;5;241m=\u001b[39mmemory)\n\u001b[1;32m      6\u001b[0m initial_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need to build a data warehouse that integrates data from our ERP and CRM systems \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto improve reporting and decision-making. Ask me any questions to clarify our requirements. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen you have all the information you need, simply reply with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOP\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mrun(initial_prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConversationChain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "initial_prompt = (\n",
    "    \"We need to build a data warehouse that integrates data from our ERP and CRM systems \"\n",
    "    \"to improve reporting and decision-making. Ask me any questions to clarify our requirements. \"\n",
    "    \"When you have all the information you need, simply reply with 'STOP'.\"\n",
    ")\n",
    "\n",
    "response = conversation.run(initial_prompt)\n",
    "print(\"LLM:\", response)\n",
    "\n",
    "while True:\n",
    "    # Ask the next question or prompt\n",
    "    next_output = conversation.run(\"Next question?\")\n",
    "    print(\"LLM:\", next_output)\n",
    "    \n",
    "    # Check if the LLM signals that it is done\n",
    "    if \"STOP\" in next_output.upper():\n",
    "        print(\"LLM decided to stop asking questions.\")\n",
    "        break\n",
    "\n",
    "    # (Optional) Simulate user responses here. In a real application, you might capture user input.\n",
    "    user_input = input(\"Your answer: \")\n",
    "    conversation.run(user_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
